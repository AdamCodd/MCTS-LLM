{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-07T14:59:43.326Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install -qU transformers sentence-transformers\n",
    "!pip install scipy spacy nltk textstat\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-07T14:59:43.328Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from requests.exceptions import RequestException\n",
    "from typing import Optional, Type\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class SingletonMeta(type):\n",
    "    _instances = {}\n",
    "    def __call__(cls, *args, **kwargs):\n",
    "        if cls not in cls._instances:\n",
    "            cls._instances[cls] = super().__call__(*args, **kwargs)\n",
    "        return cls._instances[cls]\n",
    "\n",
    "class APIManager(metaclass=SingletonMeta):\n",
    "    def __init__(self):\n",
    "        self.session = requests.Session()\n",
    "        \n",
    "    def _api_request(self, method, url, **kwargs):\n",
    "        try:\n",
    "            response = self.session.request(method, url, **kwargs)\n",
    "            response.raise_for_status()\n",
    "            return response.json()\n",
    "        except RequestException as e:\n",
    "            raise Exception(f'API request error: {e}')\n",
    "            \n",
    "    def run(self, prompt: str, temperature: float = 0.0, api_url: str = \"http://localhost:8000\",\n",
    "            model_schema: Optional[Type[BaseModel]] = None): \n",
    "        \"\"\"\n",
    "        Send completion request to vLLM API server\n",
    "        \n",
    "        Parameters:\n",
    "        - prompt: text prompt to generate from\n",
    "        - temperature: controls randomness (0.0 = deterministic)\n",
    "        - api_url: URL of the vLLM API server (on Kaggle use ngrok to get the reverse proxy URL)\n",
    "        - output_model: optional pydantic model to validate the JSON output (e.g., ScoreModel)\n",
    "        \"\"\"\n",
    "        request_payload = {\n",
    "            'prompt': prompt,\n",
    "            'max_tokens': 2048,\n",
    "            'temperature': temperature,\n",
    "            'top_p': 0.9,\n",
    "            'min_p': 0.0,\n",
    "            'top_k': 0,\n",
    "            'typical_p': 1.0,\n",
    "            'tfs': 1.0,\n",
    "            'top_a': 0.0,\n",
    "            'repetition_penalty': 1.0,\n",
    "            'min_new_tokens': 200,\n",
    "            'no_repeat_ngram_size': 0,\n",
    "            'num_beams': 1,\n",
    "            'seed': -1,\n",
    "            'add_bos_token': True,\n",
    "            'truncation_length': 8192,\n",
    "            'ban_eos_token': False,\n",
    "            'skip_special_tokens': True,\n",
    "        }\n",
    "\n",
    "        # Add output_schema only if model_schema is provided\n",
    "        if model_schema:\n",
    "            request_payload['output_schema'] = model_schema.model_json_schema()\n",
    "        \n",
    "        response = self._api_request('POST', f'{api_url}/v1/completions', json=request_payload)\n",
    "        generated_text = response['choices'][0]['text']\n",
    "\n",
    "        if model_schema:\n",
    "            try:\n",
    "                # When using model_schema, the response is guaranteed to be valid JSON\n",
    "                return json.loads(generated_text)\n",
    "            except json.JSONDecodeError as e:\n",
    "                # This should only happen if there's an unexpected issue with the API\n",
    "                raise ValueError(f\"Failed to parse response as JSON: {e}. Response: {generated_text[:100]}...\")\n",
    "        else:\n",
    "            return generated_text\n",
    "\n",
    "api_manager = APIManager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-07T14:59:43.328Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import json\n",
    "import numpy as np\n",
    "import re\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import statistics\n",
    "from collections import deque, defaultdict\n",
    "from difflib import SequenceMatcher\n",
    "from IPython.display import display\n",
    "import networkx as nx\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Union, List\n",
    "%matplotlib inline\n",
    "\n",
    "# For Metrics\n",
    "from collections import Counter\n",
    "from itertools import tee\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.spatial.distance import cosine\n",
    "import spacy\n",
    "import nltk\n",
    "nltk.download('punkt') # Not needed on Kaggle\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from textstat import flesch_reading_ease\n",
    "from textblob import TextBlob\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "## Pydantic models for guided decoding with vLLM\n",
    "class ScoreModel(BaseModel):\n",
    "    score: int = Field(..., ge=0, le=100, description=\"Score between 0 and 100\")\n",
    "\n",
    "class FeedbackModel(BaseModel):\n",
    "    issues: Union[str, List[str]]\n",
    "\n",
    "class LLMInterface:\n",
    "    INSTRUCTION_SET = 'llama3'\n",
    "\n",
    "    @classmethod\n",
    "    def ask_llm_remote(cls, prompt, model_schema=None):\n",
    "        topic = PromptManager.get_topic()\n",
    "        temperature = 0.5\n",
    "        if topic == 'creative':\n",
    "            temperature = 1.0\n",
    "        try:\n",
    "            full_response = api_manager.run(prompt, temperature=temperature, model_schema=model_schema)\n",
    "        except Exception as e:\n",
    "            print(f\"Error calling LLM API: {e}\")\n",
    "            return None\n",
    "\n",
    "        response_delimiters = {\n",
    "            'alpaca': \"### Response:\\n\",\n",
    "            'vicuna': \"### ASSISTANT:\",\n",
    "            'llama3': \"assistant\\n\\n\",\n",
    "            'chatml': \"<|im_start|>assistant\\n\"\n",
    "        }\n",
    "        response_delimiter = response_delimiters[cls.INSTRUCTION_SET]\n",
    "        response = full_response.split(response_delimiter)[-1].strip() if response_delimiter in full_response else full_response\n",
    "        return response\n",
    "    \n",
    "    @classmethod\n",
    "    def replace_placeholders(cls, prompt_parts):\n",
    "        \"\"\" Replace placeholders in the prompt with the selected instruction set \"\"\"\n",
    "        instruction_sets = {\n",
    "            \"alpaca\": {\n",
    "                \"instruction\": \"### Instruction:\\n\",\n",
    "                \"input\": \"### Input:\\n\",\n",
    "                \"response\": \"### Response:\\n\"\n",
    "            },\n",
    "            \"vicuna\": {\n",
    "                \"instruction\": \"### USER:\\n\",\n",
    "                \"response\": \"### ASSISTANT:\\n\"\n",
    "                # No \"input\" for Vicuna\n",
    "            },\n",
    "            \"llama3\": {\n",
    "                \"instruction\": \"<|start_header_id|>system<|end_header_id|>\\n\\n\",\n",
    "                \"input\": \"<|start_header_id|>user<|end_header_id|>\\n\\n\",\n",
    "                \"response\": \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
    "                \"end\": \"<|eot_id|>\\n\"\n",
    "            },\n",
    "            \"chatml\": {\n",
    "                \"instruction\": \"<|im_start|>system\\n\",\n",
    "                \"input\": \"<|im_start|>user\\n\",\n",
    "                \"response\": \"<|im_start|>assistant\\n\",\n",
    "                \"end\": \"<|im_end|>\\n\"\n",
    "            }\n",
    "        }\n",
    "\n",
    "        selected_set = instruction_sets.get(cls.INSTRUCTION_SET)\n",
    "        if not selected_set:\n",
    "            raise ValueError(f\"Instruction set {cls.INSTRUCTION_SET} is not defined.\")\n",
    "\n",
    "        if isinstance(prompt_parts, tuple):\n",
    "            prompt_parts = [prompt_parts]\n",
    "\n",
    "        final_prompt = \"\"\n",
    "        prepend_response = \"\"\n",
    "        for text, part in prompt_parts:\n",
    "            if part == \"response\":\n",
    "                prepend_response = text\n",
    "            if part in selected_set:\n",
    "                final_prompt += f\"{selected_set[part]}{text}\"\n",
    "                if \"end\" in selected_set:\n",
    "                    final_prompt += selected_set['end']\n",
    "            elif part == \"input\" and \"input\" not in selected_set:\n",
    "                # Merge input part with the instruction for sets that do not support \"input\"\n",
    "                final_prompt += f\"{text}\"\n",
    "                if \"end\" in selected_set:\n",
    "                    final_prompt += selected_set['end']\n",
    "\n",
    "        final_prompt += f\"{selected_set['response']}{prepend_response}\"\n",
    "        return final_prompt\n",
    "\n",
    "    @classmethod\n",
    "    def set_instruction_set(cls, instruction_set):\n",
    "        if instruction_set in [\"alpaca\", \"vicuna\", \"llama3\", \"chatml\"]:\n",
    "            cls.INSTRUCTION_SET = instruction_set\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported instruction set: {instruction_set}\")\n",
    "\n",
    "            \n",
    "class Node:\n",
    "    node_counter = 0\n",
    "    \n",
    "    def __init__(self, query, answer, feedback=None, refined_answer=None, parent=None):\n",
    "        self.id = self.generate_id(parent)\n",
    "        self.query = query\n",
    "        self.answer = answer\n",
    "        self.feedback = feedback\n",
    "        self.refined_answer = refined_answer\n",
    "        self.parent = parent\n",
    "        self.children = set()\n",
    "        self.visits = 0\n",
    "        self.rewards = deque(maxlen=100)  # Store only the last 100 rewards\n",
    "        self.reward_sum = 0\n",
    "        self.reward_sum_squared = 0\n",
    "        self.Q_value = 0\n",
    "        self.previous_Q_value = 0\n",
    "        self.max_children = 5\n",
    "        self.importance_weight = 1.0\n",
    "        self.depth = 0 if parent is None else parent.depth + 1\n",
    "\n",
    "    @classmethod\n",
    "    def generate_id(cls, parent):\n",
    "        if parent is None:\n",
    "            cls.node_counter += 1\n",
    "            return str(cls.node_counter - 1)\n",
    "        else:\n",
    "            parent_id = parent.id\n",
    "            child_number = len(parent.children) + 1\n",
    "            return f\"{parent_id}.{child_number}\"\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"Node_{self.id}\"        \n",
    "        \n",
    "        \n",
    "    def increment_visits(self):\n",
    "        self.visits += 1\n",
    "\n",
    "    def update_Q(self):\n",
    "        # Update previous Q-value\n",
    "        self.previous_Q_value = self.Q_value\n",
    "\n",
    "        # Formula from the paper based on the minimum and average of the rewards\n",
    "        if self.rewards:\n",
    "            min_reward = min(self.rewards)\n",
    "            avg_reward = sum(self.rewards) / len(self.rewards)\n",
    "            self.Q_value = 0.5 * (min_reward + avg_reward)\n",
    "\n",
    "        else:\n",
    "            # If there are no rewards, and no children, default to Q-value of 0\n",
    "            if not self.children:\n",
    "                self.Q_value = 0\n",
    "            else:\n",
    "                # Optionally include children's Q-values\n",
    "                self.Q_value = max(child.Q_value for child in self.children)\n",
    "\n",
    "        # Ensure Q-value stays within the range\n",
    "        self.Q_value = max(0, min(100, self.Q_value))\n",
    "\n",
    "    def add_reward(self, reward):\n",
    "        if len(self.rewards) == self.rewards.maxlen:\n",
    "            old_reward = self.rewards[0]\n",
    "            self.reward_sum -= old_reward\n",
    "            self.reward_sum_squared -= old_reward ** 2\n",
    "        \n",
    "        self.rewards.append(reward)\n",
    "        self.reward_sum += reward\n",
    "        self.reward_sum_squared += reward ** 2 \n",
    "\n",
    "    def get_ancestors(self):\n",
    "        \"\"\" Collect all ancestor nodes for a given node. \"\"\"\n",
    "        ancestors = []\n",
    "        current = self.parent\n",
    "        while current:\n",
    "            ancestors.append(current)\n",
    "            current = current.parent\n",
    "        return ancestors    \n",
    "\n",
    "    def analyze_historical_performance(self):\n",
    "        \"\"\" Analyze the feedback of ancestors to identify common issues using fuzzy matching. \"\"\"\n",
    "        ancestors = self.get_ancestors()[:5]  # Limit to 5 most recent ancestors\n",
    "        issues = defaultdict(float)\n",
    "\n",
    "        def normalize_text(text):\n",
    "            return re.sub(r'[^\\w\\s]', '', text.lower())\n",
    "\n",
    "        def fuzzy_match(a, b, threshold=0.8):\n",
    "            return SequenceMatcher(None, normalize_text(a), normalize_text(b)).ratio() > threshold\n",
    "\n",
    "        for i, ancestor in enumerate(ancestors):\n",
    "            if ancestor.feedback and 'issues' in ancestor.feedback:\n",
    "                weight = 1 / (i + 1)  # More recent ancestors have higher weight\n",
    "                for issue in ancestor.feedback['issues']:\n",
    "                    issue_text = issue if isinstance(issue, str) else str(issue)\n",
    "                    issue_text = normalize_text(issue_text)\n",
    "                    matched = False\n",
    "                    for existing_issue in issues:\n",
    "                        if fuzzy_match(issue_text, existing_issue):\n",
    "                            issues[existing_issue] += weight\n",
    "                            matched = True\n",
    "                            break\n",
    "                    if not matched:\n",
    "                        issues[issue_text] = weight\n",
    "\n",
    "        if issues:\n",
    "            common_issues = sorted(issues.items(), key=lambda x: x[1], reverse=True)[:3]\n",
    "            # Format in a structured way\n",
    "            formatted_issues = \"\\n\".join([f\"- {issue}\" for issue, _ in common_issues])\n",
    "            return f\"[Common Issues]\\n{formatted_issues}\"\n",
    "        return None\n",
    "\n",
    "    def generate_feedback(self):\n",
    "        self.feedback = PromptManager.generate_feedback(self.query, self.answer)\n",
    "\n",
    "    def refine_answer(self):\n",
    "        historical_insights = self.analyze_historical_performance()\n",
    "        historical_context = \"\"\n",
    "        if historical_insights:\n",
    "            historical_context = f\"Note: Previous answers often struggled with:\\n{historical_insights}.\"\n",
    "\n",
    "        if self.parent:\n",
    "            base_answer = self.parent.refined_answer if self.parent.refined_answer else self.parent.answer\n",
    "        else:\n",
    "            base_answer = self.answer if self.answer else \"No previous answer available.\"\n",
    "            \n",
    "        refined_answer = PromptManager.refine_answer(self.query, base_answer, self.feedback, historical_context)\n",
    "        if refined_answer is not None:\n",
    "            self.refined_answer = refined_answer\n",
    "\n",
    "    def self_evaluate(self, scoring_method, evaluator):\n",
    "        \"\"\"Evaluate the node using the provided evaluator instance.\"\"\"\n",
    "        return evaluator.evaluate_node(node=self, scoring_method=scoring_method)\n",
    "            \n",
    "    def create_child(self):\n",
    "        \"\"\"Create a new child node.\"\"\"\n",
    "        new_node = Node(\n",
    "            query=self.query,\n",
    "            answer=self.refined_answer if self.refined_answer else self.answer,\n",
    "            feedback=None,\n",
    "            refined_answer=None,\n",
    "            parent=self\n",
    "        )\n",
    "        self.children.add(new_node)\n",
    "        return new_node\n",
    "\n",
    "## Preprocess the prompt before sending it to the LLM\n",
    "class PromptManager:\n",
    "    topic = \"creative\"  # Default topic\n",
    "    terminology = {\n",
    "        \"math\": {\"task\": \"query\", \"response\": \"Answer\"},\n",
    "        \"creative\": {\"task\": \"Task\", \"response\": \"Story\"}\n",
    "    }\n",
    "\n",
    "    @classmethod\n",
    "    def set_topic(cls, new_topic):\n",
    "        if new_topic in cls.terminology:\n",
    "            cls.topic = new_topic\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported topic: {new_topic}\")\n",
    "\n",
    "    @classmethod\n",
    "    def get_topic(cls):\n",
    "        return cls.topic      \n",
    "            \n",
    "    @classmethod\n",
    "    def get_terminology(cls):\n",
    "        return cls.terminology[cls.topic]\n",
    "    \n",
    "    @classmethod\n",
    "    def generate_feedback_prompt(cls, task, response):\n",
    "        terms = cls.get_terminology()\n",
    "        feedback_context = \"Review the response critically and identify areas for improvement.\"\n",
    "        prompt_parts = [\n",
    "            (f\"{terms['task']}: {task}\\n{terms['response']}: {response}\\n\", \"instruction\"),\n",
    "            (feedback_context + \"\\nProvide feedback in the following JSON format: {\\\"issues\\\": [\\\"issue1\\\", \\\"issue2\\\", ...]}\\n\", \"input\")\n",
    "        ]\n",
    "        return LLMInterface.replace_placeholders(prompt_parts).lstrip()\n",
    "    \n",
    "    @staticmethod\n",
    "    def format_json_with_tags(json_data):\n",
    "        \"\"\"Format JSON data with markup tags based on the keys.\"\"\"\n",
    "        if not json_data:\n",
    "            return \"\"\n",
    "            \n",
    "        formatted_text = \"\"\n",
    "        for key, value in json_data.items():\n",
    "            formatted_text += f\"[{key.replace('_', ' ').title()}]\\n\"\n",
    "            if isinstance(value, list):\n",
    "                formatted_text += \"\\n\".join([f\"- {item}\" for item in value]) + \"\\n\"\n",
    "            else:\n",
    "                formatted_text += f\"{value}\\n\"\n",
    "        return formatted_text.strip()\n",
    "    \n",
    "    @classmethod\n",
    "    def generate_refine_prompt(cls, task, base_response, feedback, historical_context=\"\"):\n",
    "        terms = cls.get_terminology()\n",
    "        topic_instructions = {\n",
    "            \"math\": \"Based on the previous response, solve the query. Start by reasoning step by step within <think></think>, then verify with <verify></verify>, finally give the final answer <answer></answer>.\",\n",
    "            \"creative\": \"Based on the previous rejected draft, improve your work. Start by reasoning step by step within <think></think> then output the full story <story></story>.\"\n",
    "        }\n",
    "        \n",
    "        instruction = topic_instructions.get(cls.topic, \"No specific instructions available for the chosen topic.\")\n",
    "        \n",
    "        # Format feedback with markup tags instead of using raw JSON\n",
    "        formatted_feedback = cls.format_json_with_tags(feedback)\n",
    "        \n",
    "        # historical context is put at the end of the context to emphasis the repeated changes to make\n",
    "        if historical_context:\n",
    "            prompt = [\n",
    "                (f\"{terms['task']}: {task}\\n\", \"instruction\"),\n",
    "                (f\"{terms['response']}:\\n{base_response}\\n[FEEDBACK]:\\n{formatted_feedback}\\n{historical_context}\\n{instruction}\\n\", \"input\")\n",
    "            ]\n",
    "        else:\n",
    "            prompt = [\n",
    "                (f\"{terms['task']}: {task}\\n\", \"instruction\"),\n",
    "                (f\"{terms['response']}:\\n{base_response}\\n[FEEDBACK]:\\n{formatted_feedback}\\n{instruction}\\n\", \"input\")\n",
    "            ]\n",
    "        return LLMInterface.replace_placeholders(prompt).lstrip()\n",
    "\n",
    "    @staticmethod\n",
    "    def extract_structured_data(response, expected_key, match_all=False):\n",
    "        \"\"\"\n",
    "        Parse JSON response using Pydantic models based on the expected key\n",
    "        \"\"\"\n",
    "        if isinstance(response, dict):\n",
    "            # If response is already a dict (from using model_schema), extract directly\n",
    "            return response.get(expected_key)\n",
    "            \n",
    "        if not isinstance(response, str):\n",
    "            response = str(response)\n",
    "        \n",
    "        # Try to find JSON content\n",
    "        json_pattern = r'\\{[\\s\\S]+\\}'\n",
    "        match = re.search(json_pattern, response)\n",
    "        \n",
    "        if not match:\n",
    "            return None\n",
    "            \n",
    "        json_str = match.group()\n",
    "        \n",
    "        try:\n",
    "            # Use different Pydantic models based on the expected key\n",
    "            if expected_key == 'issues':\n",
    "                parsed_data = FeedbackModel.model_validate_json(json_str)\n",
    "                return parsed_data.issues\n",
    "            elif expected_key == 'score':\n",
    "                parsed_data = ScoreModel.model_validate_json(json_str)\n",
    "                return parsed_data.score\n",
    "            else:\n",
    "                # Fall back to basic JSON parsing for other keys\n",
    "                data = json.loads(json_str)\n",
    "                return data.get(expected_key)\n",
    "        except Exception:\n",
    "            # If parsing fails, return None\n",
    "            return None\n",
    "\n",
    "    @classmethod\n",
    "    def generate_feedback(cls, task, response):\n",
    "        feedback_prompt = cls.generate_feedback_prompt(task, response)\n",
    "        print(\"--------------\\nFeedback prompt\\n--------------\\n\", feedback_prompt)\n",
    "    \n",
    "        attempts = 0\n",
    "        valid_feedback = False\n",
    "        feedback_answer = {}\n",
    "    \n",
    "        while not valid_feedback and attempts < 5:\n",
    "            raw_feedback = LLMInterface.ask_llm_remote(feedback_prompt, FeedbackModel)\n",
    "            print(f\"Feedback (Attempt {attempts + 1}/5):\\n{raw_feedback}\")\n",
    "    \n",
    "            # When using FeedbackModel schema, raw_feedback is guaranteed to be valid\n",
    "            # We can directly use it without additional validation\n",
    "            if isinstance(raw_feedback, dict) and 'issues' in raw_feedback:\n",
    "                feedback_answer = raw_feedback\n",
    "                valid_feedback = True\n",
    "            else:\n",
    "                print(\"Unexpected feedback format. Retrying...\")\n",
    "    \n",
    "            attempts += 1\n",
    "    \n",
    "        if not valid_feedback:\n",
    "            print(\"Failed to obtain valid structured feedback after multiple attempts. Using default feedback.\")\n",
    "            feedback_answer = {'issues': ['Unable to generate specific feedback.']}\n",
    "    \n",
    "        print(\"--------------\\nFeedback answer\\n--------------\\n\", feedback_answer)\n",
    "        return feedback_answer\n",
    "\n",
    "    @classmethod\n",
    "    def refine_answer(cls, task, base_response, feedback, historical_context=\"\"):\n",
    "        refine_prompt = cls.generate_refine_prompt(task, base_response, feedback, historical_context)\n",
    "        print(\"--------------\\nRefine prompting\\n--------------\\n\", refine_prompt)\n",
    "        refined_response = LLMInterface.ask_llm_remote(refine_prompt).lstrip()\n",
    "        attempts = 0\n",
    "    \n",
    "        while attempts < 3:\n",
    "            if cls.topic == \"creative\":\n",
    "                # Look for think tags with possible surrounding characters\n",
    "                think_pattern = re.compile(r'.*?<think>.*?</think>.*?', re.DOTALL)\n",
    "                \n",
    "                # Look for story tags with possible surrounding characters\n",
    "                story_start_pattern = re.compile(r'.*?<story>(.*)', re.DOTALL)\n",
    "                story_end_pattern = re.compile(r'(.*?)</story>.*', re.DOTALL)\n",
    "                \n",
    "                has_think = bool(think_pattern.match(refined_response))\n",
    "                story_start_match = story_start_pattern.match(refined_response)\n",
    "                \n",
    "                if has_think and story_start_match:\n",
    "                    # We found a story start, now find where it ends\n",
    "                    partial_content = story_start_match.group(1)\n",
    "                    story_end_match = story_end_pattern.match(partial_content)\n",
    "                    \n",
    "                    if story_end_match:\n",
    "                        # We have both tags, extract the content\n",
    "                        story_content = story_end_match.group(1).lstrip()\n",
    "                        refined_response = story_content\n",
    "                        break\n",
    "            \n",
    "            elif cls.topic == \"maths\":\n",
    "                # Look for think, verify, and answer tags with possible surrounding characters\n",
    "                think_pattern = re.compile(r'.*?<think>.*?</think>.*?', re.DOTALL)\n",
    "                verify_pattern = re.compile(r'.*?<verify>.*?</verify>.*?', re.DOTALL)\n",
    "                \n",
    "                # Get answer content specifically\n",
    "                answer_start_pattern = re.compile(r'.*?<answer>(.*)', re.DOTALL)\n",
    "                answer_end_pattern = re.compile(r'(.*?)</answer>.*', re.DOTALL)\n",
    "                \n",
    "                has_think = bool(think_pattern.match(refined_response))\n",
    "                has_verify = bool(verify_pattern.match(refined_response))\n",
    "                answer_start_match = answer_start_pattern.match(refined_response)\n",
    "                \n",
    "                if has_think and has_verify and answer_start_match:\n",
    "                    # We found an answer start, now find where it ends\n",
    "                    partial_content = answer_start_match.group(1)\n",
    "                    answer_end_match = answer_end_pattern.match(partial_content)\n",
    "                    \n",
    "                    if answer_end_match:\n",
    "                        # We have all required tags, extract the content\n",
    "                        answer_content = answer_end_match.group(1).lstrip()\n",
    "                        refined_response = answer_content\n",
    "                        break\n",
    "            \n",
    "            # If we get here, required markers weren't found\n",
    "            attempts += 1\n",
    "            print(f\"Attempt nÂ°{attempts} - Required markup not found. Resending reprompting for the refine answer\")\n",
    "            refined_response = LLMInterface.ask_llm_remote(refine_prompt).lstrip()\n",
    "            print(\"--------------\\nRefined response\\n--------------\\n\", refined_response)\n",
    "        \n",
    "        print(\"--------------\\nRefined response\\n--------------\\n\", refined_response)\n",
    "        return refined_response\n",
    "\n",
    "    @classmethod\n",
    "    def generate_evaluation_prompt(cls, task, refined_response):\n",
    "        terms = cls.get_terminology()\n",
    "        prompt = [\n",
    "            (f\"{terms['task']}:{task}\\n{terms['response']}:{refined_response}\\n\", \"instruction\"),\n",
    "            (\"As an expert, analyze the output critically, then give it a score. The score must be a number between 0 and 100. JSON format: {\\\"score\\\": 0}\\n\", \"input\")\n",
    "        ]\n",
    "        return LLMInterface.replace_placeholders(prompt)\n",
    "        \n",
    "## Evaluate the answer through different metrics\n",
    "class Evaluator:\n",
    "    def __init__(self, topic=\"creative\"):\n",
    "        self.topic = topic\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self._gpt2_model = None\n",
    "        self._gpt2_tokenizer = None\n",
    "        self._tfidf = None\n",
    "        self._nlp = None\n",
    "        self._sentence_model = None\n",
    "\n",
    "        # Set metrics configuration based on topic\n",
    "        self.configure_metrics_for_topic()\n",
    "\n",
    "    def configure_metrics_for_topic(self):\n",
    "        \"\"\"Configure metrics weights and flags based on the topic\"\"\"\n",
    "        # Default configuration\n",
    "        # Computation intensive : llm_autoeval > coherence > perplexity > diversity > entity_consistency > sentiment_consistency > readability\n",
    "        self.metrics_config = {\n",
    "            'perplexity': True,\n",
    "            'readability': True,\n",
    "            'coherence': True,\n",
    "            'entity_consistency': True,\n",
    "            'sentiment_consistency': True,\n",
    "            'diversity': True,\n",
    "            'llm_autoeval': True\n",
    "        }\n",
    "        \n",
    "        # Default weights\n",
    "        self.metric_weights = {\n",
    "            'perplexity': 0.05,\n",
    "            'readability': 0.10,\n",
    "            'coherence': 0.25,\n",
    "            'entity_consistency': 0.20,\n",
    "            'sentiment_consistency': 0.05,\n",
    "            'diversity': 0.15,\n",
    "            'llm_autoeval': 0.20\n",
    "        }\n",
    "        \n",
    "        # Adjust weights and enable/disable metrics based on topic\n",
    "        if self.topic == \"math\":\n",
    "            # For math, prioritize accuracy and clarity\n",
    "            self.metrics_config.update({\n",
    "                'perplexity': True,\n",
    "                'readability': True,\n",
    "                'coherence': True,\n",
    "                'entity_consistency': True,\n",
    "                'sentiment_consistency': False,  # Less relevant for math\n",
    "                'diversity': False,             # Less relevant for math\n",
    "                'llm_autoeval': True\n",
    "            })\n",
    "            \n",
    "            self.metric_weights.update({\n",
    "                'perplexity': 0.05,            \n",
    "                'readability': 0.15,           # Higher for math (clarity)\n",
    "                'coherence': 0.30,             # Higher for logical flow\n",
    "                'entity_consistency': 0.20,    # Renamed to term_consistency for math\n",
    "                'llm_autoeval': 0.30           # Higher weight on LLM evaluation\n",
    "            })\n",
    "            \n",
    "        elif self.topic == \"creative\":\n",
    "            # For creative, prioritize coherence, diversity, and readability\n",
    "            self.metrics_config.update({\n",
    "                'perplexity': True,\n",
    "                'readability': True,\n",
    "                'coherence': True,\n",
    "                'entity_consistency': True,\n",
    "                'sentiment_consistency': True,  # Important for creative\n",
    "                'diversity': True,             # Important for creative\n",
    "                'llm_autoeval': True\n",
    "            })\n",
    "            \n",
    "            self.metric_weights.update({\n",
    "                'perplexity': 0.05,\n",
    "                'readability': 0.15,           # Higher for creative writing\n",
    "                'coherence': 0.20,             # Still important but less than math\n",
    "                'entity_consistency': 0.15,    # Slightly lower than default\n",
    "                'sentiment_consistency': 0.10,  # Higher for creative writing\n",
    "                'diversity': 0.20,             # Higher for creative writing\n",
    "                'llm_autoeval': 0.15           # Moderate weight on LLM evaluation\n",
    "            })\n",
    "\n",
    "    @property\n",
    "    def gpt2_model(self):\n",
    "        if self._gpt2_model is None:\n",
    "            print(\"Loading GPT-2 model for perplexity calculation...\")\n",
    "            self._gpt2_model = GPT2LMHeadModel.from_pretrained('gpt2').to(self.device)\n",
    "        return self._gpt2_model\n",
    "\n",
    "    @property\n",
    "    def gpt2_tokenizer(self):\n",
    "        if self._gpt2_tokenizer is None:\n",
    "            self._gpt2_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "        return self._gpt2_tokenizer\n",
    "\n",
    "    @property\n",
    "    def tfidf(self):\n",
    "        if self._tfidf is None:\n",
    "            self._tfidf = TfidfVectorizer()\n",
    "        return self._tfidf\n",
    "\n",
    "    @property\n",
    "    def nlp(self):\n",
    "        if self._nlp is None:\n",
    "            print(\"Loading spaCy model...\")\n",
    "            self._nlp = spacy.load(\"en_core_web_sm\")\n",
    "        return self._nlp\n",
    "\n",
    "    @property\n",
    "    def sentence_model(self):\n",
    "        if self._sentence_model is None:\n",
    "            print(\"Loading SentenceTransformer model for coherence...\")\n",
    "            self._sentence_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        return self._sentence_model\n",
    "    \n",
    "    def calculate_perplexity(self, text):\n",
    "        max_length = 1024\n",
    "        sentences = sent_tokenize(text)\n",
    "        chunks = []\n",
    "        current_chunk = []\n",
    "\n",
    "        for sentence in sentences:\n",
    "            sentence_tokens = self.gpt2_tokenizer.encode(sentence, add_special_tokens=False)\n",
    "            if len(current_chunk) + len(sentence_tokens) > max_length:\n",
    "                chunks.append(current_chunk)\n",
    "                current_chunk = sentence_tokens\n",
    "            else:\n",
    "                current_chunk.extend(sentence_tokens)\n",
    "\n",
    "        if current_chunk:\n",
    "            chunks.append(current_chunk)\n",
    "\n",
    "        total_loss = 0\n",
    "        total_length = 0\n",
    "\n",
    "        for chunk in chunks:\n",
    "            inputs = torch.tensor(chunk, dtype=torch.long).unsqueeze(0).to(self.device)\n",
    "            with torch.no_grad():\n",
    "                outputs = self.gpt2_model(inputs, labels=inputs)\n",
    "            total_loss += outputs.loss.item() * len(chunk)\n",
    "            total_length += len(chunk)\n",
    "\n",
    "        average_loss = total_loss / total_length\n",
    "        raw_perplexity = np.exp(average_loss)\n",
    "\n",
    "        # Normalize perplexity\n",
    "        max_perplexity = 100\n",
    "        log_perplexity = np.log1p(raw_perplexity)  # Log transformation\n",
    "        normalized_perplexity = 100 * (1 - log_perplexity / np.log1p(max_perplexity))  # Normalize to 0-100\n",
    "\n",
    "        return max(0, min(100, normalized_perplexity))  # Ensure it stays within [0, 100]\n",
    "\n",
    "    def calculate_diversity(self, text):        \n",
    "        doc = self.nlp(text.lower())\n",
    "        tokens = [token.text for token in doc if not token.is_punct and not token.is_space]\n",
    "        \n",
    "        if not tokens:\n",
    "            return 0\n",
    "        \n",
    "        # Distinct unigrams\n",
    "        unigrams = set(tokens)\n",
    "        unigram_diversity = len(unigrams) / len(tokens) if tokens else 0\n",
    "        \n",
    "        # Distinct bigrams\n",
    "        bigrams = list(self.ngrams(tokens, 2))\n",
    "        distinct_bigrams = set(bigrams)\n",
    "        bigram_diversity = len(distinct_bigrams) / len(bigrams) if bigrams else 0\n",
    "        \n",
    "        # Distinct trigrams\n",
    "        trigrams = list(self.ngrams(tokens, 3))\n",
    "        distinct_trigrams = set(trigrams)\n",
    "        trigram_diversity = len(distinct_trigrams) / len(trigrams) if trigrams else 0\n",
    "        \n",
    "        # Type-Token Ratio (TTR)\n",
    "        ttr = len(set(tokens)) / len(tokens) if tokens else 0\n",
    "        \n",
    "        # Vocabulary richness (using hapax legomena)\n",
    "        vocab = Counter(tokens)\n",
    "        hapax = len([word for word, freq in vocab.items() if freq == 1])\n",
    "        hapax_ratio = hapax / len(tokens) if tokens else 0\n",
    "        \n",
    "        # Shannon entropy for unigrams\n",
    "        token_counts = Counter(tokens)\n",
    "        token_probs = [count / len(tokens) for count in token_counts.values()]\n",
    "        unigram_entropy = -sum(p * math.log2(p) for p in token_probs)\n",
    "        \n",
    "        # Shannon entropy for bigrams\n",
    "        if bigrams:\n",
    "            bigram_counts = Counter(bigrams)\n",
    "            bigram_probs = [count / len(bigrams) for count in bigram_counts.values()]\n",
    "            bigram_entropy = -sum(p * math.log2(p) for p in bigram_probs)\n",
    "        else:\n",
    "            bigram_entropy = 0\n",
    "        \n",
    "        # Shannon entropy for trigrams\n",
    "        if trigrams:\n",
    "            trigram_counts = Counter(trigrams)\n",
    "            trigram_probs = [count / len(trigrams) for count in trigram_counts.values()]\n",
    "            trigram_entropy = -sum(p * math.log2(p) for p in trigram_probs)\n",
    "        else:\n",
    "            trigram_entropy = 0\n",
    "        \n",
    "        # Max possible entropy is log2(num_unique_tokens)\n",
    "        max_unigram_entropy = math.log2(len(unigrams)) if unigrams else 0\n",
    "        max_bigram_entropy = math.log2(len(distinct_bigrams)) if distinct_bigrams else 0\n",
    "        max_trigram_entropy = math.log2(len(distinct_trigrams)) if distinct_trigrams else 0\n",
    "        \n",
    "        # Normalized entropy (between 0 and 1)\n",
    "        norm_unigram_entropy = unigram_entropy / max_unigram_entropy if max_unigram_entropy else 0\n",
    "        norm_bigram_entropy = bigram_entropy / max_bigram_entropy if max_bigram_entropy else 0\n",
    "        norm_trigram_entropy = trigram_entropy / max_trigram_entropy if max_trigram_entropy else 0\n",
    "        \n",
    "        # Calculate the diversity score including both original metrics and entropy\n",
    "        diversity_score = (unigram_diversity + bigram_diversity + trigram_diversity + \n",
    "                            ttr + hapax_ratio + \n",
    "                            norm_unigram_entropy + norm_bigram_entropy + norm_trigram_entropy) / 8\n",
    "        \n",
    "        return diversity_score\n",
    "\n",
    "    @staticmethod\n",
    "    def ngrams(tokens, n):\n",
    "        iterables = tee(tokens, n)\n",
    "        for i, sub_iterable in enumerate(iterables):\n",
    "            for _ in range(i):\n",
    "                next(sub_iterable, None)\n",
    "        return zip(*iterables)\n",
    "    \n",
    "    def calculate_readability(self, text):\n",
    "        raw_score = flesch_reading_ease(text)\n",
    "        # Clamp the score to a reasonable range before normalizing\n",
    "        normalized_score = max(0, min(100, raw_score))\n",
    "        return normalized_score\n",
    "        \n",
    "    def calculate_coherence(self, text):\n",
    "        sentences = sent_tokenize(text)\n",
    "\n",
    "        # Fall back to original method for very short texts\n",
    "        if len(sentences) < 3:\n",
    "            return self.calculate_coherence_original(text)\n",
    "\n",
    "        # Use sentence embeddings instead of TF-IDF\n",
    "        embeddings = self.sentence_model.encode(sentences)\n",
    "\n",
    "        # Calculate pairwise cosine similarities\n",
    "        similarities = cosine_similarity(embeddings)\n",
    "\n",
    "        # Calculate coherence scores\n",
    "        coherence_scores = []\n",
    "        for i in range(len(sentences)):\n",
    "            # Compare each sentence with all others, excluding self-comparison\n",
    "            sentence_scores = similarities[i, [j for j in range(len(sentences)) if j != i]]\n",
    "            avg_score = np.mean(sentence_scores)\n",
    "            coherence_scores.append(avg_score)\n",
    "\n",
    "        # Calculate overall coherence\n",
    "        overall_coherence = np.mean(coherence_scores)\n",
    "\n",
    "        # Check for NaN or infinite values\n",
    "        if np.isnan(overall_coherence) or np.isinf(overall_coherence):\n",
    "            print(\"Warning: Invalid overall coherence. Returning default score.\")\n",
    "            return 50  # Return a middle score as a fallback\n",
    "\n",
    "        # Ensure overall_coherence is within [-1, 1] range\n",
    "        overall_coherence = max(-1, min(1, overall_coherence))\n",
    "\n",
    "        # Normalize to 0-1 range\n",
    "        normalized_coherence = (overall_coherence + 1) / 2\n",
    "\n",
    "        return normalized_coherence\n",
    "\n",
    "    def calculate_coherence_original(self, text):\n",
    "        # Keep the original method for fallback and comparison\n",
    "        sentences = sent_tokenize(text)\n",
    "        sentence_vectors = self.tfidf.fit_transform(sentences)\n",
    "        coherence_scores = []\n",
    "        for i in range(len(sentences) - 1):\n",
    "            vec1 = sentence_vectors[i].toarray().flatten()\n",
    "            vec2 = sentence_vectors[i+1].toarray().flatten()\n",
    "\n",
    "            if np.all(vec1 == 0) or np.all(vec2 == 0):\n",
    "                similarity = 0\n",
    "            else:\n",
    "                similarity = 1 - cosine(vec1, vec2)\n",
    "\n",
    "            coherence_scores.append(similarity)\n",
    "        return np.mean(coherence_scores) if coherence_scores else 0\n",
    "\n",
    "    def calculate_entity_consistency(self, text):\n",
    "        doc = self.nlp(text)\n",
    "        entities = [ent.text.lower() for ent in doc.ents]\n",
    "        unique_entities = set(entities)\n",
    "        consistency = len(unique_entities) / len(entities) if entities else 0\n",
    "        return 1 - consistency\n",
    "\n",
    "    def calculate_sentiment_consistency(self, text):\n",
    "        sentences = sent_tokenize(text)\n",
    "        sentiments = [TextBlob(sentence).sentiment.polarity for sentence in sentences]\n",
    "        return 1 - np.std(sentiments)\n",
    "\n",
    "    def evaluate_node(self, node, base_evaluations=3, min_evaluations=3, attempts_per_evaluation=3, scoring_method='lowest'):\n",
    "        \"\"\" Evaluate the node's refined answer using configurable objective quality metrics \"\"\"\n",
    "        num_evaluations = max(min_evaluations, base_evaluations - node.depth)\n",
    "        valid_scores = []\n",
    "        weights = []\n",
    "\n",
    "        # Calculate metric scores once\n",
    "        metric_scores = {}\n",
    "        \n",
    "        # Only consider weights of active metrics\n",
    "        active_weights = {metric: weight \n",
    "                         for metric, weight in self.metric_weights.items() \n",
    "                         if self.metrics_config.get(metric, False)}\n",
    "        total_weight = sum(active_weights.values())\n",
    "    \n",
    "        if self.metrics_config.get('perplexity', False):\n",
    "            perplexity = self.calculate_perplexity(node.refined_answer)\n",
    "            metric_scores['perplexity'] = 100 * (1 / (1 + perplexity))\n",
    "\n",
    "        if self.metrics_config.get('readability', False):\n",
    "            metric_scores['readability'] = self.calculate_readability(node.refined_answer)\n",
    "\n",
    "        if self.metrics_config.get('coherence', False):\n",
    "            metric_scores['coherence'] = self.calculate_coherence(node.refined_answer) * 100\n",
    "\n",
    "        if self.metrics_config.get('entity_consistency', False):\n",
    "            metric_scores['entity_consistency'] = self.calculate_entity_consistency(node.refined_answer) * 100\n",
    "\n",
    "        if self.metrics_config.get('sentiment_consistency', False):\n",
    "            metric_scores['sentiment_consistency'] = self.calculate_sentiment_consistency(node.refined_answer) * 100\n",
    "\n",
    "        if self.metrics_config.get('diversity', False):\n",
    "            metric_scores['diversity'] = self.calculate_diversity(node.refined_answer) * 100\n",
    "\n",
    "        # LLM-based evaluations\n",
    "        llm_final_score = None\n",
    "        if self.metrics_config.get('llm_autoeval', False):\n",
    "            for evaluation in range(num_evaluations):\n",
    "                llm_score = self.get_llm_score(node, attempts_per_evaluation)\n",
    "                if llm_score is not None:\n",
    "                    valid_scores.append(llm_score)\n",
    "                    weights.append(num_evaluations - evaluation)  # Weight based on evaluation number\n",
    "                print(f\"Evaluation {evaluation + 1}: LLM Score = {llm_score}\")\n",
    "\n",
    "            if valid_scores:\n",
    "                if scoring_method == 'highest':\n",
    "                    llm_final_score = max(valid_scores)\n",
    "                elif scoring_method == 'lowest':\n",
    "                    llm_final_score = min(valid_scores)\n",
    "                elif scoring_method == 'average':\n",
    "                    llm_final_score = sum(valid_scores) / len(valid_scores)\n",
    "                elif scoring_method == 'weighted_average':\n",
    "                    llm_final_score = sum(score * weight for score, weight in zip(valid_scores, weights)) / sum(weights)\n",
    "                elif scoring_method == 'median':\n",
    "                    llm_final_score = statistics.median(valid_scores)\n",
    "                else:\n",
    "                    raise ValueError(f\"Unknown scoring method: {scoring_method}\")\n",
    "    \n",
    "        # Combine all scores using only active metric weights\n",
    "        all_scores = [score * active_weights[metric] for metric, score in metric_scores.items()]\n",
    "        if llm_final_score is not None:\n",
    "            all_scores.append(llm_final_score * active_weights['llm_autoeval'])\n",
    "    \n",
    "        if all_scores:\n",
    "            final_score = sum(all_scores) / total_weight\n",
    "\n",
    "            if final_score > 95:\n",
    "                final_score -= 5  # Curb excessive scores\n",
    "\n",
    "            node.add_reward(final_score)\n",
    "            score_json = json.dumps({'score': final_score, 'metrics': metric_scores, 'llm_score': llm_final_score})\n",
    "            print(f\"Final score for Node_{node.id}: {score_json}\")\n",
    "            return score_json\n",
    "        else:\n",
    "            default_score = 0  # Maintaining the original default score\n",
    "            node.add_reward(default_score)\n",
    "            score_json = json.dumps({'score': default_score})\n",
    "            print(f\"Failed to obtain any valid scores for Node_{node.id}. Default score: {score_json}\")\n",
    "            return score_json\n",
    "\n",
    "    def get_llm_score(self, node, attempts_per_evaluation):\n",
    "        for attempt in range(attempts_per_evaluation):\n",
    "            evaluation_prompt = PromptManager.generate_evaluation_prompt(node.query, node.refined_answer)\n",
    "            response = LLMInterface.ask_llm_remote(evaluation_prompt, ScoreModel)\n",
    "            print(\"Auto-eval:\", response)\n",
    "            \n",
    "            # When using ScoreModel, the response is guaranteed to have a 'score' field\n",
    "            if isinstance(response, dict) and 'score' in response:\n",
    "                score = response['score']\n",
    "                if 0 <= score <= 100:\n",
    "                    return score\n",
    "        return None\n",
    "    \n",
    "class ImportanceSampler:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def update_importance_weights(self, node):\n",
    "        \"\"\"\n",
    "        Recursively traverse the tree from the given node and update the\n",
    "        importance weights of its children based on their Q-values.\n",
    "        \"\"\"\n",
    "        if not node.children:\n",
    "            return\n",
    "\n",
    "        # Collect the Q-values of all direct children of the current node.\n",
    "        child_q_values = [child.Q_value for child in node.children]\n",
    "        total_q_value = sum(child_q_values)\n",
    "\n",
    "        # Update the importance weight for each child.\n",
    "        for child in node.children:\n",
    "            if total_q_value > 0:\n",
    "                # Calculate the weight as the child's share of the total Q-value.\n",
    "                child.importance_weight = child.Q_value / total_q_value\n",
    "            else:\n",
    "                # If all children have a Q-value of 0, they have equal importance.\n",
    "                child.importance_weight = 1.0 / len(node.children)\n",
    "\n",
    "            # Recursively call the update function for the child node to update its own children.\n",
    "            self.update_importance_weights(child)    \n",
    "\n",
    "    \n",
    "# Use matplot to display the tree graph + nodes' Q_value\n",
    "class MCTSVisualizer:\n",
    "    def __init__(self):\n",
    "        self.fig, (self.ax1, self.ax2) = plt.subplots(1, 2, figsize=(20, 10))\n",
    "        self.all_nodes = []\n",
    "        self.iterations = []\n",
    "        \n",
    "    def update(self, iteration, tree):\n",
    "        self.iterations.append(iteration)\n",
    "        self.all_nodes = self._get_all_nodes(tree.root)\n",
    "\n",
    "        self.ax1.clear()\n",
    "        self.ax2.clear()\n",
    "\n",
    "        self._plot_q_values()\n",
    "        self._plot_tree(tree)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "\n",
    "        display(self.fig)\n",
    "\n",
    "    def _get_all_nodes(self, node):\n",
    "        nodes = [node]\n",
    "        for child in node.children:\n",
    "            nodes.extend(self._get_all_nodes(child))\n",
    "        return nodes\n",
    "\n",
    "    def _plot_q_values(self):\n",
    "        for node in self.all_nodes:\n",
    "            self.ax1.scatter(self.iterations[-1], node.Q_value, alpha=0.5)\n",
    "            self.ax1.annotate(f\"Node {node.id}\", (self.iterations[-1], node.Q_value), \n",
    "                              xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "        self.ax1.set_title(\"Q-values of all nodes over iterations\")\n",
    "        self.ax1.set_xlabel(\"Iteration\")\n",
    "        self.ax1.set_ylabel(\"Q-value\")\n",
    "\n",
    "    def _plot_tree(self, tree):\n",
    "        G = nx.Graph()\n",
    "        self._build_graph(tree.root, G)\n",
    "        pos = nx.spring_layout(G)\n",
    "        nx.draw(G, pos, ax=self.ax2, with_labels=False, node_size=30)\n",
    "        self.ax2.set_title(\"MCTS Tree Structure\")\n",
    "\n",
    "        # Add node labels\n",
    "        labels = {node: f\"{node.id}\\n{node.Q_value:.2f}\" for node in G.nodes()}\n",
    "        nx.draw_networkx_labels(G, pos, labels, font_size=8, ax=self.ax2)\n",
    "\n",
    "    def _build_graph(self, node, G):\n",
    "        G.add_node(node)\n",
    "        for child in node.children:\n",
    "            G.add_edge(node, child)\n",
    "            self._build_graph(child, G)\n",
    "    \n",
    "    def save_figure(self, filename=\"mcts_visualization.png\"):\n",
    "        self.fig.savefig(filename, bbox_inches='tight', dpi=300)\n",
    "        print(f\"Figure saved as {filename}\")\n",
    "            \n",
    "class MCTSTree:\n",
    "    def __init__(self, query, min_depth1_nodes=3, importance_sampling=False):\n",
    "        self.root = self.initialize_root(query, use_dummy=False)\n",
    "        self.min_depth1_nodes = min_depth1_nodes\n",
    "        self.use_importance_sampling = importance_sampling\n",
    "        self.importance_sampler = ImportanceSampler() if importance_sampling else None\n",
    "        \n",
    "    def initialize_root(self, query, use_dummy=False):\n",
    "        \"\"\" Initialize the tree with either a dummy or a model-generated answer at the root \"\"\"\n",
    "        answer = \"\"\n",
    "        if use_dummy:\n",
    "            answer = \"I don't know how to solve this query.\"\n",
    "        else:\n",
    "            query_f = (f\"{query}\", \"instruction\")\n",
    "            query_formatted = LLMInterface.replace_placeholders(query_f)\n",
    "            answer = LLMInterface.ask_llm_remote(query_formatted)\n",
    "        print(\"--------------\\nRoot answer\\n--------------\\n\", answer)\n",
    "        root_node = Node(query, answer)\n",
    "        return root_node                   \n",
    "\n",
    "    def select_node(self, current_node):\n",
    "        \"\"\"Select the most promising node to explore next.\"\"\"\n",
    "        if current_node == self.root and len(current_node.children) < self.min_depth1_nodes:\n",
    "            print(f\"Expanding root node: Node_{current_node.id} (depth: {current_node.depth})\")\n",
    "            print(f\"Q-value = {current_node.Q_value:.4f}, Previous Q-value = {current_node.previous_Q_value:.4f}, Visits = {current_node.visits}\")\n",
    "            print(f\"Number of children: {len(current_node.children)}\")\n",
    "            print(f\"Max children allowed: {current_node.max_children}\")\n",
    "            return current_node  # Always expand root until min_depth1_nodes is reached\n",
    "\n",
    "        while current_node.children:\n",
    "            if len(current_node.children) < current_node.max_children:\n",
    "                unexpanded_child = self.select_candidate_nodes([current_node] + list(current_node.children))\n",
    "                if unexpanded_child == current_node:\n",
    "                    return current_node  # Expand this node\n",
    "                else:\n",
    "                    current_node = unexpanded_child  # Move to the selected child\n",
    "            else:\n",
    "                current_node = self.select_candidate_nodes(list(current_node.children))\n",
    "        return current_node\n",
    "\n",
    "    def select_candidate_nodes(self, nodes, exploration_weight=1.5):\n",
    "        if not nodes:\n",
    "            return None\n",
    "\n",
    "        total_visits = sum(node.visits for node in nodes)\n",
    "        ucb_values = []\n",
    "        for node in nodes:\n",
    "            if node.visits == 0:\n",
    "                ucb = float('inf')  # Ensure unvisited nodes are selected first\n",
    "            else:\n",
    "                exploitation = node.Q_value\n",
    "                parent_visits = node.parent.visits if node.parent else total_visits\n",
    "                exploration = exploration_weight * math.sqrt(math.log(parent_visits + 1) / (node.visits))\n",
    "                if self.use_importance_sampling:\n",
    "                    importance_weight = node.importance_weight\n",
    "                    ucb = (exploitation + exploration) * importance_weight\n",
    "                else:\n",
    "                    ucb = exploitation + exploration\n",
    "            ucb_values.append((node, ucb))\n",
    "            print(f\"Node_{node.id} (depth: {node.depth}): UCB value = {ucb:.4f} (Q-value = {node.Q_value:.4f}, Previous Q-value = {node.previous_Q_value:.4f}, Visits = {node.visits})\")\n",
    "\n",
    "        selected_node, selected_ucb = max(ucb_values, key=lambda x: x[1])\n",
    "        print(f\"Selected Node_{selected_node.id} with UCB value {selected_ucb:.4f}\")\n",
    "        print(f\"Number of children of the selected node: {len(selected_node.children)}\")\n",
    "        print(f\"Max children allowed of the selected node: {selected_node.max_children}\")\n",
    "        return selected_node\n",
    "    \n",
    "    def expand_node(self, node):\n",
    "        \"\"\"Expand the selected node by creating a new child.\"\"\"\n",
    "        print(\"Creating new child node\")\n",
    "        return node.create_child()\n",
    "\n",
    "    def simulate(self, node, evaluator):\n",
    "        \"\"\"Simulate the node's performance.\"\"\"\n",
    "        node.generate_feedback()\n",
    "        node.refine_answer()\n",
    "        node.self_evaluate('lowest', evaluator)\n",
    "\n",
    "    def termination_check(self, node, current_iteration, max_iterations=8, threshold=80):\n",
    "        if current_iteration >= max_iterations:\n",
    "            return True, \"Maximum iterations reached\"\n",
    "        elif node.Q_value >= threshold:  # Threshold for high-quality solution\n",
    "            return True, \"[Early Stopping] High quality solution found\"\n",
    "        return False, \"\"\n",
    "\n",
    "    def backpropagate(self, node):\n",
    "        while node is not None:\n",
    "            node.increment_visits()\n",
    "            old_Q = node.Q_value\n",
    "\n",
    "            # Initially update Q-value based on its own rewards (handled in update_Q)\n",
    "            node.update_Q()\n",
    "\n",
    "            # Now adjust Q-value based on the formula Q'(a)\n",
    "            if node.children:\n",
    "                max_child_Q = max(child.Q_value for child in node.children)\n",
    "                node.Q_value = 0.5 * (node.Q_value + max_child_Q)\n",
    "\n",
    "            # If Q-value barely changed, stop propagation\n",
    "            if abs(old_Q - node.Q_value) < 1e-3:\n",
    "                break\n",
    "            node = node.parent\n",
    "\n",
    "    def iterate_tree(self, node):\n",
    "        \"\"\"Iterate through all nodes in the tree.\"\"\"\n",
    "        yield node\n",
    "        for child in node.children:\n",
    "            yield from self.iterate_tree(child)         \n",
    "               \n",
    "    def get_best_path(self):\n",
    "        \"\"\"Get the path to the node with the highest Q-value in the entire tree.\"\"\"\n",
    "        best_node = self.find_best_node(self.root)\n",
    "        path = []\n",
    "        current = best_node\n",
    "        while current:\n",
    "            path.append(current)\n",
    "            current = current.parent\n",
    "        return list(reversed(path))  # Reverse to get path from root to best node\n",
    "\n",
    "    def find_best_node(self, node):\n",
    "        \"\"\"Recursively find the node with the highest Q-value in the subtree.\"\"\"\n",
    "        best_node = node\n",
    "        for child in node.children:\n",
    "            child_best = self.find_best_node(child)\n",
    "            if child_best.Q_value > best_node.Q_value:\n",
    "                best_node = child_best\n",
    "        return best_node\n",
    "    \n",
    "\n",
    "# Save the MCTS state at any iteration and restart from that point\n",
    "class MCTSStateManager:\n",
    "    @staticmethod\n",
    "    def save_state(tree, iteration, filename):\n",
    "        state = {\n",
    "            'iteration': iteration,\n",
    "            'tree': MCTSStateManager._serialize_tree(tree.root)\n",
    "        }\n",
    "        with open(filename, 'w') as f:\n",
    "            json.dump(state, f, indent=2)\n",
    "        print(f\"State saved to {filename}\")\n",
    "\n",
    "    @staticmethod\n",
    "    def load_state(filename):\n",
    "        if not os.path.exists(filename):\n",
    "            print(f\"No saved state found at {filename}\")\n",
    "            return None, 0\n",
    "\n",
    "        with open(filename, 'r') as f:\n",
    "            state = json.load(f)\n",
    "        \n",
    "        tree = MCTSTree(state['tree']['query'])\n",
    "        tree.root = MCTSStateManager._deserialize_tree(state['tree'], None)\n",
    "        return tree, state['iteration']\n",
    "\n",
    "    @staticmethod\n",
    "    def _serialize_tree(node):\n",
    "        serialized = {\n",
    "            'id': node.id,\n",
    "            'query': node.query,\n",
    "            'answer': node.answer,\n",
    "            'feedback': node.feedback,\n",
    "            'refined_answer': node.refined_answer,\n",
    "            'visits': node.visits,\n",
    "            'rewards': list(node.rewards),\n",
    "            'Q_value': node.Q_value,\n",
    "            'previous_Q_value': node.previous_Q_value,\n",
    "            'max_children': node.max_children,\n",
    "            'importance_weight': node.importance_weight,\n",
    "            'depth': node.depth,\n",
    "            'children': [MCTSStateManager._serialize_tree(child) for child in node.children]\n",
    "        }\n",
    "        return json.loads(json.dumps(serialized, ensure_ascii=False))\n",
    "\n",
    "    @staticmethod\n",
    "    def _deserialize_tree(data, parent):\n",
    "        node = Node(\n",
    "            query=data['query'],\n",
    "            answer=data['answer'],\n",
    "            feedback=data['feedback'],\n",
    "            refined_answer=data['refined_answer'],\n",
    "            parent=parent\n",
    "        )\n",
    "        node.id = data['id']\n",
    "        node.visits = data['visits']\n",
    "        node.rewards = deque(data['rewards'], maxlen=100)\n",
    "        node.Q_value = data['Q_value']\n",
    "        node.previous_Q_value = data['previous_Q_value']\n",
    "        node.max_children = data['max_children']\n",
    "        node.importance_weight = data['importance_weight']\n",
    "        node.depth = data['depth']\n",
    "\n",
    "        for child_data in data['children']:\n",
    "            child = MCTSStateManager._deserialize_tree(child_data, node)\n",
    "            node.children.add(child)\n",
    "\n",
    "        return node    \n",
    "    \n",
    "    \n",
    "def mcts_sr_algorithm(query, topic, min_depth1_nodes=3, iterations=8, qvalue_threshold=80, importance_sampling=False, save_interval=4, load_file=None):\n",
    "    # Set the topic for PromptManager\n",
    "    PromptManager.set_topic(topic)  \n",
    "    \n",
    "    # Single instance of Evaluator will be reused for all node evaluations.\n",
    "    evaluator = Evaluator(topic=topic)\n",
    "    \n",
    "    # Load state if a load file is provided\n",
    "    if load_file:\n",
    "        tree, start_iteration = MCTSStateManager.load_state(load_file)\n",
    "        print(f\"Loading the MCTS from previous state from iteration {start_iteration}\")\n",
    "        if tree is None:\n",
    "            print(\"Tree not found in the file. Starting over at Iteration #1\")\n",
    "            tree = MCTSTree(query, min_depth1_nodes=min_depth1_nodes, importance_sampling=importance_sampling)\n",
    "            tree.simulate(tree.root, evaluator)\n",
    "            tree.backpropagate(tree.root)\n",
    "            start_iteration = 0\n",
    "    else:\n",
    "        tree = MCTSTree(query, min_depth1_nodes=min_depth1_nodes, importance_sampling=importance_sampling)\n",
    "        start_iteration = 0\n",
    "        # Simulate and backpropagate root node for new tree\n",
    "        tree.simulate(tree.root, evaluator)\n",
    "        tree.backpropagate(tree.root)\n",
    "    \n",
    "    visualizer = MCTSVisualizer()\n",
    "    termination_reason = None\n",
    "\n",
    "    for i in range(start_iteration, iterations):\n",
    "        print(f\"\\n---- Iteration {i + 1} ----\")\n",
    "        \n",
    "        # Selection\n",
    "        selected_node = tree.select_node(tree.root)\n",
    "        selected_node.increment_visits()\n",
    "        \n",
    "        # Expansion\n",
    "        if len(selected_node.children) < selected_node.max_children:\n",
    "            new_node = tree.expand_node(selected_node)\n",
    "            \n",
    "            # Simulation\n",
    "            tree.simulate(new_node, evaluator)\n",
    "\n",
    "            # Backpropagation\n",
    "            tree.backpropagate(new_node)\n",
    "        else:\n",
    "            print(\"Node fully expanded, backtracking...\")\n",
    "\n",
    "        # Update importance weights if using importance sampling\n",
    "        if importance_sampling:\n",
    "            tree.importance_sampler.update_importance_weights(tree.root)\n",
    "\n",
    "        # Check for termination conditions\n",
    "        terminated, reason = tree.termination_check(selected_node, i + 1, max_iterations=iterations, threshold=qvalue_threshold)\n",
    "        if terminated:\n",
    "            termination_reason = reason\n",
    "            print(f\"Termination condition met: {reason}\")\n",
    "            break\n",
    "\n",
    "        # Update visualization\n",
    "        visualizer.update(i+1, tree)\n",
    "\n",
    "        # Save state at specified intervals if save_interval is not set to 0\n",
    "        if save_interval > 0 and (i + 1) % save_interval == 0:\n",
    "            MCTSStateManager.save_state(tree, i + 1, f\"mcts_state_iteration_{i+1}.json\")\n",
    "        \n",
    "    # If loop completes without early termination\n",
    "    if termination_reason is None:\n",
    "        termination_reason = \"Maximum iterations reached\"\n",
    "\n",
    "    # Final best path\n",
    "    final_best_path = tree.get_best_path()\n",
    "    best_node = final_best_path[-1]\n",
    "    \n",
    "    print(\"\\n---- Final Results ----\")\n",
    "    print(f\"Termination reason: {termination_reason}\")\n",
    "    print(f\"Best node: Node_{best_node.id}\")\n",
    "    print(f\"Best Q value: {best_node.Q_value}\")\n",
    "    print(f\"Best answer: {best_node.refined_answer or best_node.answer}\")\n",
    "    \n",
    "    visualizer.update(iterations, tree)\n",
    "    visualizer.save_figure()\n",
    "    \n",
    "    # Save final state as JSON\n",
    "    MCTSStateManager.save_state(tree, iterations, \"mcts_state_final.json\")\n",
    "    \n",
    "    return\n",
    "\n",
    "# Topics available: math, creative\n",
    "query = \"Write a short story about a cat finding an owner.\"\n",
    "print(\"Starting MCTS algorithm for: \", f\"'{query}'\")\n",
    "mcts_sr_algorithm(query, topic=\"creative\", min_depth1_nodes=3, iterations=5, qvalue_threshold=80, importance_sampling=False, save_interval=3, load_file=None)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30732,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
