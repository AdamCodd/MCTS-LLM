{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "execution_failed": "2025-03-01T19:34:21.706Z",
     "iopub.execute_input": "2025-03-01T18:40:28.998407Z",
     "iopub.status.busy": "2025-03-01T18:40:28.998035Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!export VLLM_NO_USAGE_STATS=1\n",
    "!export DO_NOT_TRACK=1\n",
    "!mkdir -p ~/.config/vllm && touch ~/.config/vllm/do_not_track\n",
    "\n",
    "!pip install -U vllm\n",
    "!pip install pyngrok https://download.pytorch.org/whl/nightly/pytorch_triton-3.1.0%2B5fe38ffd73-cp310-cp310-linux_x86_64.whl # Needed for Kaggle\n",
    "!ngrok authtoken YourToken # Needed for Kaggle\n",
    "\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from vllm import LLM, SamplingParams\n",
    "from vllm.sampling_params import GuidedDecodingParams\n",
    "from pyngrok import ngrok\n",
    "import uvicorn\n",
    "import nest_asyncio\n",
    "import sys\n",
    "from typing import Optional, Dict, Any\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class CompletionRequest(BaseModel):\n",
    "    prompt: str\n",
    "    output_schema: Optional[Dict[str, Any]] = None\n",
    "    max_tokens: Optional[int] = 2048\n",
    "    temperature: Optional[float] = 0.0\n",
    "    top_p: Optional[float] = 0.9\n",
    "    min_p: Optional[float] = 0.0\n",
    "    top_k: Optional[int] = 0\n",
    "    typical_p: Optional[float] = 1.0\n",
    "    tfs: Optional[float] = 1.0\n",
    "    top_a: Optional[float] = 0.0\n",
    "    repetition_penalty: Optional[float] = 1.0\n",
    "    no_repeat_ngram_size: Optional[int] = 0\n",
    "    num_beams: Optional[int] = 1\n",
    "    seed: Optional[int] = 0\n",
    "    add_bos_token: Optional[bool] = True\n",
    "    truncation_length: Optional[int] = 8192\n",
    "    ban_eos_token: Optional[bool] = False\n",
    "    skip_special_tokens: Optional[bool] = True\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "llm = LLM(\n",
    "    model=\"casperhansen/llama-3-8b-instruct-awq\",\n",
    "    dtype=\"half\",\n",
    "    trust_remote_code=True,\n",
    "    quantization=\"AWQ\",\n",
    "    speculative_config={\n",
    "        \"method\": \"ngram\",\n",
    "        \"num_speculative_tokens\": 5,\n",
    "        \"prompt_lookup_max\": 4,\n",
    "    }\n",
    ")\n",
    "\n",
    "@app.post(\"/v1/completions\")\n",
    "async def generate_completion(request: CompletionRequest):\n",
    "    try:\n",
    "        # Create base sampling parameters\n",
    "        sampling_params_kwargs = {\n",
    "            \"temperature\": request.temperature,\n",
    "            \"max_tokens\": request.max_tokens,\n",
    "            \"repetition_penalty\": request.repetition_penalty,\n",
    "            \"seed\": request.seed if request.seed != 0 else None\n",
    "        }\n",
    "\n",
    "        # Add guided decoding if schema is provided\n",
    "        if request.output_schema:\n",
    "            # Create guided decoding parameters\n",
    "            guided_decoding_params = GuidedDecodingParams(json=request.output_schema, backend=\"lm-format-enforcer\")\n",
    "            sampling_params_kwargs[\"guided_decoding\"] = guided_decoding_params\n",
    "\n",
    "        print(f\"Request: {request}\")\n",
    "        # Create SamplingParams with all parameters\n",
    "        sampling_params = SamplingParams(**sampling_params_kwargs)\n",
    "\n",
    "        # Generate output\n",
    "        outputs = llm.generate([request.prompt], sampling_params)\n",
    "        generated_text = outputs[0].outputs[0].text\n",
    "        print(f\"Generated text: {generated_text}\")\n",
    "\n",
    "        return {\n",
    "            \"choices\": [\n",
    "                {\n",
    "                    \"text\": generated_text,\n",
    "                    \"index\": 0,\n",
    "                    \"finish_reason\": \"length\" if len(generated_text) >= request.max_tokens else \"stop\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        print(f\"Traceback: {traceback.format_exc()}\")\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "# Setup ngrok\n",
    "ngrok_tunnel = ngrok.connect(8000)\n",
    "print('Public URL:', ngrok_tunnel.public_url, flush=True)\n",
    "sys.stdout.flush()\n",
    "\n",
    "# Enable nested asyncio for Jupyter notebooks\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Run the API with host set to 0.0.0.0 to accept external connections\n",
    "uvicorn.run(app, host=\"0.0.0.0\", port=8000)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
